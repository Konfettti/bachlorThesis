{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kunjo\\bachlorThesis\n"
     ]
    }
   ],
   "source": [
    "cd ../.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Ideas\n",
    "\n",
    "- User features:\n",
    "    - `locale`\n",
    "    - `age`\n",
    "    - `gender`\n",
    "    - `days_on_app`\n",
    "    - `location`\n",
    "    - `timezone`\n",
    "    - `num_friends`\n",
    "- For monthly windows between 1 and 5 months ago calculate the following features:\n",
    "    - `num_invited` (as per the `event_attendees` table)\n",
    "    - `num_yes`\n",
    "    - `num_no`\n",
    "    - `num_maybe`\n",
    "    - `avg_event_start_hour`\n",
    "    - `modal_event_dow`\n",
    "    - `num_invites` (as per the `event_interest` table)\n",
    "    - `num_interested`\n",
    "    - `num_not_interested`\n",
    "    - `num_invited_and_interested`\n",
    "    - `num_invited_and_not_interested`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from torch_frame.utils import infer_df_stype\n",
    "\n",
    "import utils\n",
    "from jinja2 import Template\n",
    "\n",
    "conn = duckdb.connect('event/event.db')\n",
    "%load_ext sql\n",
    "%sql conn --alias duckdb\n",
    "%config SqlMagic.displaycon=False\n",
    "%config SqlMagic.autopandas=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating train table\n",
      "train table created\n",
      "Creating val table\n",
      "val table created\n",
      "Creating test table\n",
      "test table created\n"
     ]
    }
   ],
   "source": [
    "with open('bachlorThesis/baselines/userattendance.sql', 'r') as f:\n",
    "    # run once with train_labels and once with val_labels\n",
    "    template = f.read()\n",
    "    \n",
    "def render_jinja_sql(template_text, context):\n",
    "    return Template(template_text).render(**context)\n",
    "    \n",
    "# create train, val and test features\n",
    "# takes 1 - 5 mins\n",
    "for s in ['train', 'val', 'test']:\n",
    "    print(f'Creating {s} table')\n",
    "    query = render_jinja_sql(template, dict(set=s, subsample=0))\n",
    "    conn.sql(query)\n",
    "    print(f'{s} table created')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'utils' has no attribute 'validate_feature_tables'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_feature_tables\u001b[49m(\u001b[33m'\u001b[39m\u001b[33muser_attendance\u001b[39m\u001b[33m'\u001b[39m, conn)\n",
      "\u001b[31mAttributeError\u001b[39m: module 'utils' has no attribute 'validate_feature_tables'"
     ]
    }
   ],
   "source": [
    "utils.validate_feature_tables('user_attendance', conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql train_df <<\n",
    "from user_attendance_train_feats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infer_df_stype(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.feature_summary_df(train_df, 'target', classification=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import shap\n",
    "from torch_frame import TaskType, stype\n",
    "from torch_frame.data import Dataset\n",
    "from torch_frame.gbdt import LightGBM\n",
    "\n",
    "from inferred_stypes import task_to_stypes\n",
    "from train_gbdt import TASK_PARAMS\n",
    "\n",
    "TASK = 'rel-event-user-attendance'\n",
    "\n",
    "task_params = TASK_PARAMS[TASK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql val_df <<\n",
    "select * from user_attendance_val_feats;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_stype = task_to_stypes[TASK].copy()\n",
    "del col_to_stype['title']\n",
    "del col_to_stype['last_review_summary']\n",
    "val_tf = Dataset(\n",
    "    val_df,\n",
    "    col_to_stype=col_to_stype,\n",
    "    target_col=task_params['target_col'],\n",
    ").materialize().tensor_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbdt = LightGBM(task_type=task_params['task_type'])\n",
    "gbdt.load(f'models/{TASK}_lgbm.json')\n",
    "pred = gbdt.predict(tf_test=val_tf).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "explainer = shap.TreeExplainer(gbdt.model)\n",
    "\n",
    "sample = np.random.randint(0, len(val_tf), size=10_000)\n",
    "\n",
    "val_arr, _, _ = gbdt._to_lightgbm_input(val_tf[sample])\n",
    "shap_values = explainer.shap_values(val_arr, pred[sample])\n",
    "\n",
    "# TODO verify\n",
    "feat_names = val_tf.col_names_dict.get(stype.categorical, []) + val_tf.col_names_dict[stype.numerical]\n",
    "\n",
    "shap.summary_plot(shap_values, val_arr, plot_type='violin', max_display=30, feature_names=feat_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
